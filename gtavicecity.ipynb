{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba9fd0f",
   "metadata": {},
   "source": [
    "# **Big Data Practical â€“ Simulation Notebook (Full Exam Version)**\n",
    "### **HDFS â€¢ MapReduce â€¢ Hive â€¢ Pig â€¢ Matrix Multiplication**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187f2a2",
   "metadata": {},
   "source": [
    "## **1. HDFS Simulation Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956b77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, shutil\n",
    "\n",
    "HDFS_ROOT = \"/content/hdfs\"\n",
    "os.makedirs(HDFS_ROOT, exist_ok=True)\n",
    "\n",
    "def hdfs_mkdir(path):\n",
    "    os.makedirs(HDFS_ROOT + path, exist_ok=True)\n",
    "    print(\"[HDFS] Directory Created:\", HDFS_ROOT + path)\n",
    "\n",
    "def hdfs_put(local, hdfs):\n",
    "    shutil.copy(local, HDFS_ROOT + hdfs)\n",
    "    print(\"[HDFS] Uploaded:\", local, \"â†’\", HDFS_ROOT + hdfs)\n",
    "\n",
    "def hdfs_cat(hdfs):\n",
    "    with open(HDFS_ROOT + hdfs) as f:\n",
    "        print(\"[HDFS] File Content:\\n\", f.read())\n",
    "\n",
    "def hdfs_ls(path):\n",
    "    print(\"[HDFS] Listing:\", os.listdir(HDFS_ROOT + path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bdf42a",
   "metadata": {},
   "source": [
    "## **2. HDFS File Operations (mkdir, put, cat, ls)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035214c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hdfs_mkdir(\"/exam\")\n",
    "hdfs_mkdir(\"/exam/employee\")\n",
    "\n",
    "with open(\"/content/employee.txt\", \"w\") as f:\n",
    "    f.write(\"Employee data simulation for Hadoop practical exam.\")\n",
    "\n",
    "hdfs_put(\"/content/employee.txt\", \"/exam/employee/employee.txt\")\n",
    "\n",
    "hdfs_cat(\"/exam/employee/employee.txt\")\n",
    "\n",
    "hdfs_ls(\"/exam/employee\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4edc333",
   "metadata": {},
   "source": [
    "## **3. MapReduce Simulation â€“ WordCount**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def map_wordcount(text):\n",
    "    return [(word.lower(), 1) for word in text.split()]\n",
    "\n",
    "def reduce_wordcount(mapped):\n",
    "    counts = defaultdict(int)\n",
    "    for word, val in mapped:\n",
    "        counts[word] += val\n",
    "    return counts\n",
    "\n",
    "text = \"Hadoop Map Reduce Word Count Hadoop Practical Simulation\"\n",
    "mapped = map_wordcount(text)\n",
    "reduced = reduce_wordcount(mapped)\n",
    "\n",
    "print(\"Mapped Output:\", mapped)\n",
    "print(\"Reduced Output:\", dict(reduced))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f3eb1c",
   "metadata": {},
   "source": [
    "## **4. MapReduce Simulation â€“ Matrix Multiplication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa4dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def matrix_map(A, B):\n",
    "    out=[]\n",
    "    for i in range(len(A)):\n",
    "        for k in range(len(A[0])):\n",
    "            for j in range(len(B[0])):\n",
    "                out.append(((i,j), A[i][k]*B[k][j]))\n",
    "    return out\n",
    "\n",
    "def matrix_reduce(mapped):\n",
    "    res=defaultdict(int)\n",
    "    for key,val in mapped:\n",
    "        res[key]+=val\n",
    "    return res\n",
    "\n",
    "A = [[1,2],[3,4]]\n",
    "B = [[5,6],[7,8]]\n",
    "\n",
    "mapped = matrix_map(A, B)\n",
    "reduced = matrix_reduce(mapped)\n",
    "\n",
    "print(\"Matrix Multiplication Result:\", dict(reduced))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeb29f6",
   "metadata": {},
   "source": [
    "## **5. Hive Simulation â€“ Table + Query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa70150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "employee_table = [\n",
    "    {\"id\":1, \"name\":\"Aman\", \"salary\":50000},\n",
    "    {\"id\":2, \"name\":\"Riya\", \"salary\":60000},\n",
    "    {\"id\":3, \"name\":\"John\", \"salary\":55000},\n",
    "]\n",
    "\n",
    "def hive_select(condition):\n",
    "    return [row for row in employee_table if condition(row)]\n",
    "\n",
    "high_salary = hive_select(lambda row: row[\"salary\"] > 55000)\n",
    "high_salary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0520f7",
   "metadata": {},
   "source": [
    "## **6. Pig Simulation â€“ Group & Sum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd485cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales = [\n",
    "    (\"North\", \"ProductA\", 100),\n",
    "    (\"South\", \"ProductA\", 150),\n",
    "    (\"North\", \"ProductB\", 200),\n",
    "]\n",
    "\n",
    "grouped = {}\n",
    "for region, product, amt in sales:\n",
    "    grouped.setdefault(region, []).append(amt)\n",
    "\n",
    "total_sales = {region: sum(values) for region, values in grouped.items()}\n",
    "total_sales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4682f163",
   "metadata": {},
   "source": [
    "## **Notebook Ready for Exam Submission ðŸš€**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
